{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c5314d-ff7c-4e5d-8d35-60e339050368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.json as paj\n",
    "import pyarrow.parquet as pq\n",
    "import ijson\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import io\n",
    "from typing import List, Dict, Any, Iterator\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1cc3751-fd18-469b-aaed-0ce3ebfba8c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Spark configs such as 'spark.sql.execution.arrow.pyspark.enabled' cannot be set at runtime in Databricks serverless/Spark Connect.\n",
    "# If you need these configs, set them in the cluster configuration or in the SparkSession builder in your class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c8a683-d98e-477d-b3c5-42d5b611976f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DatabricksArrowJSONSplitter:\n",
    "    \"\"\"\n",
    "    Arrow-optimized JSON.GZ splitter designed for Databricks serverless compute.\n",
    "    Uses Arrow's columnar processing and vectorized operations for maximum performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, app_name: str = \"ArrowJSONSplitter\"):\n",
    "        \"\"\"No SparkSession creation here; use global spark object.\"\"\"\n",
    "        self.app_name = app_name\n",
    "    \n",
    "    def split_nested_gz_with_arrow(self, input_files: List[str], output_path: str,\n",
    "                                  nested_configs: List[Dict], batch_size: int = 50000) -> None:\n",
    "        files_df = spark.createDataFrame(\n",
    "            [(f, i) for i, f in enumerate(input_files)],\n",
    "            [\"file_path\", \"file_id\"]\n",
    "        )\n",
    "        # ... rest of method unchanged ...\n",
    "    \n",
    "    def split_with_arrow_datasets(self, input_pattern: str, output_path: str,\n",
    "                                 json_path: str = 'item', chunk_size: int = 100000) -> None:\n",
    "        input_files = self._get_databricks_files(input_pattern)\n",
    "        files_df = spark.createDataFrame(\n",
    "            [(f,) for f in input_files], \n",
    "            [\"file_path\"]\n",
    "        )\n",
    "        # ... rest of method unchanged ...\n",
    "    \n",
    "    def split_columnar_optimized(self, input_files: List[str], output_path: str,\n",
    "                               schema_config: Dict, chunk_size: int = 200000) -> None:\n",
    "        arrow_schema = self._build_arrow_schema(schema_config)\n",
    "        files_df = spark.createDataFrame(\n",
    "            [(f,) for f in input_files],\n",
    "            [\"file_path\"]\n",
    "        )\n",
    "        return files_df\n",
    "        # ... rest of method unchanged ...\n",
    "    \n",
    "    def split_with_delta_optimization(self, input_path: str, output_path: str,\n",
    "                                    json_path: str, partition_cols: List[str] = None) -> None:\n",
    "        input_files = self._get_databricks_files(input_path)\n",
    "        files_df = spark.createDataFrame([(f,) for f in input_files], [\"file_path\"])\n",
    "        # ... rest of method unchanged ...\n",
    "    \n",
    "    def split_with_databricks_autoloader(self, input_path: str, output_path: str,\n",
    "                                       schema_config: Dict, json_path: str = 'item') -> None:\n",
    "        schema = self._build_spark_schema_from_config(schema_config)\n",
    "        streaming_df = spark \\\n",
    "            .readStream \\\n",
    "            .format(\"cloudFiles\") \\\n",
    "            .option(\"cloudFiles.format\", \"json\") \\\n",
    "            .option(\"cloudFiles.compression\", \"gzip\") \\\n",
    "            .option(\"cloudFiles.schemaLocation\", f\"{output_path}/_schemas\") \\\n",
    "            .option(\"multiline\", \"true\") \\\n",
    "            .option(\"mode\", \"PERMISSIVE\") \\\n",
    "            .schema(schema) \\\n",
    "            .load(input_path)\n",
    "        # ... rest of method unchanged ...\n",
    "    \n",
    "    def split_with_unity_catalog(self, catalog_table: str, output_table: str,\n",
    "                               json_column: str, nested_configs: List[Dict]) -> None:\n",
    "        source_df = spark.table(catalog_table)\n",
    "        # ... rest of method unchanged ...\n",
    "    \n",
    "    def _write_arrow_results_to_delta(self, results: List, output_path: str) -> None:\n",
    "        results_data = []\n",
    "        for row in results:\n",
    "            result_dict = json.loads(row.processing_result)\n",
    "            results_data.append(result_dict)\n",
    "        results_df = spark.createDataFrame(results_data)\n",
    "        results_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f\"{output_path}/processing_summary\")\n",
    "    \n",
    "    def _build_arrow_schema(self, schema_config: Dict[str, Any]) -> pa.schema:\n",
    "        \"\"\"\n",
    "        Build a PyArrow schema from the schema_config dictionary.\n",
    "        Supports nested structs and lists.\n",
    "        \"\"\"\n",
    "        def parse_field(name, field_config):\n",
    "            type_map = {\n",
    "                \"string\": pa.string(),\n",
    "                \"float64\": pa.float64(),\n",
    "                \"int64\": pa.int64(),\n",
    "                \"timestamp\": pa.timestamp('ns'),\n",
    "                \"bool\": pa.bool_(),\n",
    "            }\n",
    "            nullable = field_config.get(\"nullable\", True)\n",
    "            field_type = field_config[\"type\"]\n",
    "            if field_type in type_map:\n",
    "                return pa.field(name, type_map[field_type], nullable=nullable)\n",
    "            elif field_type == \"list\":\n",
    "                item_type = field_config[\"item_type\"]\n",
    "                if item_type in type_map:\n",
    "                    value_type = type_map[item_type]\n",
    "                elif item_type == \"struct\":\n",
    "                    value_type = pa.struct([\n",
    "                        parse_field(sub_name, sub_config)\n",
    "                        for sub_name, sub_config in field_config[\"fields\"].items()\n",
    "                    ])\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported list item_type: {item_type}\")\n",
    "                return pa.field(name, pa.list_(value_type), nullable=nullable)\n",
    "            elif field_type == \"struct\":\n",
    "                struct_type = pa.struct([\n",
    "                    parse_field(sub_name, sub_config)\n",
    "                    for sub_name, sub_config in field_config[\"fields\"].items()\n",
    "                ])\n",
    "                return pa.field(name, struct_type, nullable=nullable)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported field type: {field_type}\")\n",
    "        fields = [parse_field(name, config) for name, config in schema_config.items()]\n",
    "        return pa.schema(fields)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"No Spark session to stop; nothing to do.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e246c854-88d0-4854-9774-e31adb8b5d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Databricks-specific optimization functions\n",
    "# def setup_databricks_cluster_libraries():\n",
    "#     \"\"\"\n",
    "#     Setup script for Databricks cluster initialization.\n",
    "#     Add this to cluster init scripts.\n",
    "#     \"\"\"\n",
    "#     install_commands = [\n",
    "#         \"pip install ijson pyarrow\",\n",
    "#         \"pip install --upgrade pandas\"\n",
    "#     ]\n",
    "    \n",
    "#     return \"\\n\".join([\n",
    "#         \"#!/bin/bash\",\n",
    "#         *install_commands\n",
    "#     ])\n",
    "\n",
    "# def optimize_databricks_cluster_config():\n",
    "#     \"\"\"\n",
    "#     Recommended Databricks cluster configuration for JSON.GZ processing.\n",
    "#     \"\"\"\n",
    "#     return {\n",
    "#         \"cluster_name\": \"json-gz-processor\",\n",
    "#         \"spark_version\": \"13.3.x-scala2.12\",  # Latest LTS\n",
    "#         \"node_type_id\": \"i3.xlarge\",  # Memory optimized for decompression\n",
    "#         \"driver_node_type_id\": \"i3.xlarge\",\n",
    "#         \"autoscale\": {\n",
    "#             \"min_workers\": 2,\n",
    "#             \"max_workers\": 20\n",
    "#         },\n",
    "#         \"spark_conf\": {\n",
    "#             \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
    "#             \"spark.sql.execution.arrow.maxRecordsPerBatch\": \"50000\",\n",
    "#             \"spark.sql.adaptive.enabled\": \"true\",\n",
    "#             \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "#             \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\"\n",
    "#         },\n",
    "#         \"custom_tags\": {\n",
    "#             \"purpose\": \"json-gz-processing\",\n",
    "#             \"cost-center\": \"data-engineering\"\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "# # Example usage for Databricks\n",
    "# def databricks_example():\n",
    "#     \"\"\"Example usage optimized for Databricks serverless compute.\"\"\"\n",
    "    \n",
    "#     # Initialize with Databricks optimizations\n",
    "#     splitter = DatabricksArrowJSONSplitter(\"DataBricksJSONProcessor\")\n",
    "    \n",
    "#     try:\n",
    "#         # Example 1: Process files from DBFS/Unity Catalog\n",
    "#         input_files = [\n",
    "#             \"/Volumes/mgiglia/dev_matthew_giglia_price_transparency/landing/in-network/2025-08_040_05C0_in-network-rates_1_of_5.json.gz\"\n",
    "#         ]\n",
    "        \n",
    "#         # Schema configuration for Arrow optimization\n",
    "#         schema_config = {\n",
    "#             \"id\": {\"type\": \"string\", \"nullable\": False},\n",
    "#             \"timestamp\": {\"type\": \"timestamp\", \"nullable\": True},\n",
    "#             \"user_id\": {\"type\": \"string\", \"nullable\": True},\n",
    "#             \"event_data\": {\"type\": \"string\", \"nullable\": True},\n",
    "#             \"metadata\": {\"type\": \"list\", \"nullable\": True}\n",
    "#         }\n",
    "        \n",
    "#         # Split with columnar optimization\n",
    "#         splitter.split_columnar_optimized(\n",
    "#             input_files=input_files,\n",
    "#             output_path=\"/tmp/arrow_optimized_output\",\n",
    "#             schema_config=schema_config,\n",
    "#             chunk_size=100000\n",
    "#         )\n",
    "        \n",
    "#         # Example 2: Use Auto Loader for streaming\n",
    "#         streaming_query = splitter.split_with_databricks_autoloader(\n",
    "#             input_path=\"s3://incoming-data/*.json.gz\",\n",
    "#             output_path=\"/tmp/streaming_output\",\n",
    "#             json_path=\"events.item\"\n",
    "#         )\n",
    "        \n",
    "#         # Example 3: Unity Catalog integration\n",
    "#         splitter.split_with_unity_catalog(\n",
    "#             catalog_table=\"main.raw_data.json_files\",\n",
    "#             output_table=\"main.processed_data.split_objects\",\n",
    "#             json_column=\"file_content\",\n",
    "#             nested_configs=[\n",
    "#                 {\"json_path\": \"users.item\", \"output_subdir\": \"users\"},\n",
    "#                 {\"json_path\": \"events.item\", \"output_subdir\": \"events\"}\n",
    "#             ]\n",
    "#         )\n",
    "        \n",
    "#         # Example 4: Direct Delta Lake optimization\n",
    "#         splitter.split_with_delta_optimization(\n",
    "#             input_path=\"/databricks-datasets/nested-json/*.json.gz\",\n",
    "#             output_path=\"/tmp/delta_optimized\",\n",
    "#             json_path=\"data.records.item\",\n",
    "#             partition_cols=[\"date\", \"region\"]\n",
    "#         )\n",
    "        \n",
    "#     finally:\n",
    "#         splitter.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Print cluster configuration\n",
    "#     cluster_config = optimize_databricks_cluster_config()\n",
    "#     print(\"Recommended Databricks cluster configuration:\")\n",
    "#     print(json.dumps(cluster_config, indent=2))\n",
    "    \n",
    "#     # Run Databricks example\n",
    "#     databricks_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c19835-fb5b-46bb-8c37-0d95e786fbc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arrow schema config for in-network JSON structure\n",
    "schema_config = {\n",
    "    \"reporting_entity_name\": {\"type\": \"string\", \"nullable\": False},\n",
    "    \"reporting_entity_type\": {\"type\": \"string\", \"nullable\": False},\n",
    "    \"plan_name\": {\"type\": \"string\", \"nullable\": True},\n",
    "    \"plan_id_type\": {\"type\": \"string\", \"nullable\": True},\n",
    "    \"plan_id\": {\"type\": \"string\", \"nullable\": True},\n",
    "    \"plan_market_type\": {\"type\": \"string\", \"nullable\": True},\n",
    "    \"last_updated_on\": {\"type\": \"string\", \"nullable\": False},\n",
    "    \"version\": {\"type\": \"string\", \"nullable\": False},\n",
    "    \"provider_references\": {\n",
    "        \"type\": \"list\", \"nullable\": True, \"item_type\": \"struct\", \"fields\": {\n",
    "            \"provider_group_id\": {\"type\": \"float64\", \"nullable\": False},\n",
    "            \"provider_groups\": {\n",
    "                \"type\": \"list\", \"nullable\": True, \"item_type\": \"struct\", \"fields\": {\n",
    "                    \"npi\": {\"type\": \"list\", \"nullable\": True, \"item_type\": \"float64\"},\n",
    "                    \"tin\": {\n",
    "                        \"type\": \"struct\", \"nullable\": False, \"fields\": {\n",
    "                            \"type\": {\"type\": \"string\", \"nullable\": False},\n",
    "                            \"value\": {\"type\": \"string\", \"nullable\": False}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"location\": {\"type\": \"string\", \"nullable\": True}\n",
    "        }\n",
    "    },\n",
    "    \"in_network\": {\n",
    "        \"type\": \"list\", \"nullable\": False, \"item_type\": \"struct\", \"fields\": {\n",
    "            \"negotiation_arrangement\": {\"type\": \"string\", \"nullable\": False},\n",
    "            \"name\": {\"type\": \"string\", \"nullable\": False},\n",
    "            \"billing_code_type\": {\"type\": \"string\", \"nullable\": False},\n",
    "            \"billing_code_type_version\": {\"type\": \"string\", \"nullable\": False},\n",
    "            \"billing_code\": {\"type\": \"string\", \"nullable\": False},\n",
    "            \"description\": {\"type\": \"string\", \"nullable\": False},\n",
    "            \"negotiated_rates\": {\n",
    "                \"type\": \"list\", \"nullable\": False, \"item_type\": \"struct\", \"fields\": {\n",
    "                    \"negotiated_prices\": {\n",
    "                        \"type\": \"list\", \"nullable\": False, \"item_type\": \"struct\", \"fields\": {\n",
    "                            \"service_code\": {\"type\": \"list\", \"nullable\": True, \"item_type\": \"string\"},\n",
    "                            \"billing_class\": {\"type\": \"string\", \"nullable\": False},\n",
    "                            \"negotiated_type\": {\"type\": \"string\", \"nullable\": False},\n",
    "                            \"billing_code_modifier\": {\"type\": \"list\", \"nullable\": True, \"item_type\": \"string\"},\n",
    "                            \"negotiated_rate\": {\"type\": \"float64\", \"nullable\": False},\n",
    "                            \"expiration_date\": {\"type\": \"string\", \"nullable\": False},\n",
    "                            \"additional_information\": {\"type\": \"string\", \"nullable\": True}\n",
    "                        }\n",
    "                    },\n",
    "                    \"provider_groups\": {\n",
    "                        \"type\": \"list\", \"nullable\": True, \"item_type\": \"struct\", \"fields\": {\n",
    "                            \"npi\": {\"type\": \"list\", \"nullable\": True, \"item_type\": \"float64\"},\n",
    "                            \"tin\": {\n",
    "                                \"type\": \"struct\", \"nullable\": False, \"fields\": {\n",
    "                                    \"type\": {\"type\": \"string\", \"nullable\": False},\n",
    "                                    \"value\": {\"type\": \"string\", \"nullable\": False}\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"provider_references\": {\n",
    "                        \"type\": \"list\", \"nullable\": True, \"item_type\": \"float64\"  # provider_group_id\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"covered_services\": {\n",
    "                \"type\": \"list\", \"nullable\": True, \"item_type\": \"struct\", \"fields\": {\n",
    "                    \"billing_code_type\": {\"type\": \"string\", \"nullable\": False},\n",
    "                    \"billing_code_type_version\": {\"type\": \"string\", \"nullable\": False},\n",
    "                    \"billing_code\": {\"type\": \"string\", \"nullable\": False},\n",
    "                    \"description\": {\"type\": \"string\", \"nullable\": False}\n",
    "                }\n",
    "            },\n",
    "            \"bundled_codes\": {\n",
    "                \"type\": \"list\", \"nullable\": True, \"item_type\": \"struct\", \"fields\": {\n",
    "                    \"billing_code_type\": {\"type\": \"string\", \"nullable\": False},\n",
    "                    \"billing_code_type_version\": {\"type\": \"string\", \"nullable\": False},\n",
    "                    \"billing_code\": {\"type\": \"string\", \"nullable\": False},\n",
    "                    \"description\": {\"type\": \"string\", \"nullable\": False}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# This config can be passed to _build_arrow_schema or _build_spark_schema_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a292ab-9ef1-40f1-a804-de3b1153183d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "splitter = DatabricksArrowJSONSplitter(\"DataBricksJSONProcessor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "538c53e3-9705-4fa5-a9e6-a49b7c5cf32a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"/Volumes/mgiglia/dev_matthew_giglia_price_transparency/landing/in-network/split/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f91f35-4f77-4395-a1ff-053092784ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_files = [\n",
    "  \"/Volumes/mgiglia/dev_matthew_giglia_price_transparency/landing/in-network/2025-08_040_05C0_in-network-rates_1_of_5.json.gz\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32f3e9b-c27a-41d2-bd01-edbf90015f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # Split with columnar optimization\n",
    "display(splitter.split_columnar_optimized(\n",
    "  input_files=input_files,\n",
    "  output_path=f\"{output_dir}/arrow_optimized_output\",\n",
    "  schema_config=schema_config,\n",
    "  chunk_size=100000\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2db72247-c513-4970-b233-afdf618dc7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # Example 1: Process files from DBFS/Unity Catalog\n",
    "  input_files = [\n",
    "      \"/Volumes/mgiglia/dev_matthew_giglia_price_transparency/landing/in-network/2025-08_040_05C0_in-network-rates_1_of_5.json.gz\"\n",
    "  ]\n",
    "  \n",
    "  # # Schema configuration for Arrow optimization\n",
    "  # schema_config = {\n",
    "  #     \"id\": {\"type\": \"string\", \"nullable\": False},\n",
    "  #     \"timestamp\": {\"type\": \"timestamp\", \"nullable\": True},\n",
    "  #     \"user_id\": {\"type\": \"string\", \"nullable\": True},\n",
    "  #     \"event_data\": {\"type\": \"string\", \"nullable\": True},\n",
    "  #     \"metadata\": {\"type\": \"list\", \"nullable\": True}\n",
    "  # }\n",
    "  \n",
    "  # Split with columnar optimization\n",
    "  splitter.split_columnar_optimized(\n",
    "      input_files=input_files,\n",
    "      output_path=\"/tmp/arrow_optimized_output\",\n",
    "      schema_config=schema_config,\n",
    "      chunk_size=100000\n",
    "  )\n",
    "  \n",
    "  # Example 2: Use Auto Loader for streaming\n",
    "  streaming_query = splitter.split_with_databricks_autoloader(\n",
    "      input_path=\"s3://incoming-data/*.json.gz\",\n",
    "      output_path=\"/tmp/streaming_output\",\n",
    "      json_path=\"events.item\"\n",
    "  )\n",
    "  \n",
    "  # Example 3: Unity Catalog integration\n",
    "  splitter.split_with_unity_catalog(\n",
    "      catalog_table=\"main.raw_data.json_files\",\n",
    "      output_table=\"main.processed_data.split_objects\",\n",
    "      json_column=\"file_content\",\n",
    "      nested_configs=[\n",
    "          {\"json_path\": \"users.item\", \"output_subdir\": \"users\"},\n",
    "          {\"json_path\": \"events.item\", \"output_subdir\": \"events\"}\n",
    "      ]\n",
    "  )\n",
    "  \n",
    "  # Example 4: Direct Delta Lake optimization\n",
    "  splitter.split_with_delta_optimization(\n",
    "      input_path=\"/databricks-datasets/nested-json/*.json.gz\",\n",
    "      output_path=\"/tmp/delta_optimized\",\n",
    "      json_path=\"data.records.item\",\n",
    "      partition_cols=[\"date\", \"region\"]\n",
    "  )\n",
    "  \n",
    "finally:\n",
    "  splitter.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "ijson==3.4.*"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Untitled Notebook 2025-08-26 07_59_52",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
