{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c51488-d9e1-42d8-b8d7-31acb2fb45bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ijson\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda82a81-838c-4799-9efd-f9f1835973fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def stream_and_split_gzipped_json(input_path, output_prefix, chunk_size=1000):\n",
    "    if not isinstance(input_path, str):\n",
    "        raise TypeError(\"input_path must be a string\")\n",
    "    # Open gzipped file for streaming parsing\n",
    "    with gzip.open(input_path, 'rb') as f:\n",
    "        objects = ijson.items(f, 'root.item')  # adjust 'root.item' depending on root type\n",
    "        batch = []\n",
    "        file_num = 0\n",
    "        for obj in objects:\n",
    "            batch.append(obj)\n",
    "            if len(batch) >= chunk_size:\n",
    "                with open(f\"{output_prefix}_part_{file_num}.json\", \"w\") as out_f:\n",
    "                    json.dump(batch, out_f)\n",
    "                batch = []\n",
    "                file_num += 1\n",
    "        if batch:  # flush leftovers\n",
    "            with open(f\"{output_prefix}_part_{file_num}.json\", \"w\") as out_f:\n",
    "                json.dump(batch, out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0c2476-52fc-429b-b3ef-91bbb8422c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_partition(pdf):\n",
    "    results = []\n",
    "    for path in pdf[\"file_path\"]:\n",
    "        if isinstance(path, str):\n",
    "            stream_and_split_gzipped_json(path, \"/Volumes/mgiglia/dev_matthew_giglia_price_transparency/landing/in-network/output_partitioned\", 1000)\n",
    "            results.append({\"file_path\": path, \"status\": \"done\"})\n",
    "        else:\n",
    "            results.append({\"file_path\": str(path), \"status\": \"error: path is not a string\"})\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda2bb98-9e2c-4c59-bfcc-4d1e2f923160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# On Databricks, using Spark with Arrow optimization:\n",
    "input_file_paths = [\"/Volumes/mgiglia/dev_matthew_giglia_price_transparency/landing/in-network/2025-08_040_05C0_in-network-rates_1_of_5.json.gz\"]  # list of file paths to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a09aca9-36a6-403b-ba09-8709997d12ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"file_path\", StringType()),\n",
    "    StructField(\"status\", StringType())\n",
    "])\n",
    "df = spark.createDataFrame([(path,) for path in input_file_paths], [\"file_path\"]).mapInArrow(\n",
    "    process_partition,\n",
    "    schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5469c0f0-d8ed-4167-be58-8c95e55744ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "ijson==3.4.*"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Untitled Notebook 2025-08-26 09_13_28",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
