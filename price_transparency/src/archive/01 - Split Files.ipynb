{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207f3680-720f-41b4-9253-8a39fda96ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nested_data = \"/Volumes/mgiglia/dev_matthew_giglia_price_transparency/landing/in-network/2025-08_040_05C0_in-network-rates_1_of_5.json.gz\"\n",
    "output_dir = \"/Volumes/mgiglia/dev_matthew_giglia_price_transparency/landing/in-network/split/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1953d50-856c-4157-a092-d212b60d99b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ijson\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e66bcb-d0b2-4ca9-b04c-5a6946299c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DecimalEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, Decimal):\n",
    "            return float(o)\n",
    "        return super().default(o)\n",
    "\n",
    "# When writing JSON, use:\n",
    "# json.dump(\n",
    "#     data,\n",
    "#     file,\n",
    "#     cls=DecimalEncoder,\n",
    "#     indent=2,\n",
    "#     ensure_ascii=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61b8e13-5966-4ff8-ac73-c7bf9e62b254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class NestedJSONGZSplitter:\n",
    "    \"\"\"\n",
    "    Advanced splitter for nested JSON structures in compressed files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_file: str, output_dir: str):\n",
    "        self.input_file = input_file\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.is_compressed = input_file.endswith('.gz')\n",
    "    \n",
    "    def analyze_structure(self, max_depth=3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze the JSON structure to help identify splitting paths.\n",
    "        \"\"\"\n",
    "        print(f\"Analyzing structure of {Path(self.input_file).name}...\")\n",
    "        \n",
    "        file_opener = gzip.open if self.is_compressed else open\n",
    "        structure_info = {\n",
    "            'root_type': None,\n",
    "            'possible_paths': [],\n",
    "            'sample_data': {}\n",
    "        }\n",
    "        \n",
    "        with file_opener(self.input_file, 'rb') as file:\n",
    "            try:\n",
    "                # Try to determine root structure\n",
    "                events = ijson.parse(file)\n",
    "                depth = 0\n",
    "                current_path = []\n",
    "                array_paths = []\n",
    "                \n",
    "                for prefix, event, value in events:\n",
    "                    if event == 'start_array':\n",
    "                        if prefix:\n",
    "                            array_paths.append(f\"{prefix}.item\")\n",
    "                        else:\n",
    "                            array_paths.append(\"item\")\n",
    "                            structure_info['root_type'] = 'array'\n",
    "                    elif event == 'start_map' and not prefix:\n",
    "                        structure_info['root_type'] = 'object'\n",
    "                    \n",
    "                    # Limit analysis to avoid processing entire file\n",
    "                    if len(array_paths) > 10:\n",
    "                        break\n",
    "                \n",
    "                structure_info['possible_paths'] = array_paths[:10]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing structure: {e}\")\n",
    "        \n",
    "        return structure_info\n",
    "    \n",
    "    def split_nested_array(self, json_path: str, chunk_size: int = 1000, \n",
    "                          preserve_structure: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Split nested arrays with option to preserve parent structure.\n",
    "        \n",
    "        Args:\n",
    "            json_path: IJSon path like 'data.records.item' or 'users.item'\n",
    "            chunk_size: Objects per output file\n",
    "            preserve_structure: If True, maintains the nested structure in output\n",
    "        \"\"\"\n",
    "        print(f\"Splitting nested path: {json_path}\")\n",
    "        \n",
    "        file_opener = gzip.open if self.is_compressed else open\n",
    "        current_chunk = []\n",
    "        file_counter = 1\n",
    "        total_objects = 0\n",
    "        \n",
    "        with file_opener(self.input_file, 'rb') as file:\n",
    "            parser = ijson.items(file, json_path)\n",
    "            \n",
    "            for obj in parser:\n",
    "                current_chunk.append(obj)\n",
    "                total_objects += 1\n",
    "                \n",
    "                if total_objects % 5000 == 0:\n",
    "                    print(f\"Processed {total_objects} objects from {json_path}\")\n",
    "                \n",
    "                if len(current_chunk) >= chunk_size:\n",
    "                    if preserve_structure:\n",
    "                        self._write_structured_chunk(current_chunk, json_path, file_counter)\n",
    "                    else:\n",
    "                        self._write_flat_chunk(current_chunk, file_counter)\n",
    "                    \n",
    "                    current_chunk = []\n",
    "                    file_counter += 1\n",
    "            \n",
    "            # Write final chunk\n",
    "            if current_chunk:\n",
    "                if preserve_structure:\n",
    "                    self._write_structured_chunk(current_chunk, json_path, file_counter)\n",
    "                else:\n",
    "                    self._write_flat_chunk(current_chunk, file_counter)\n",
    "        \n",
    "        print(f\"Split {total_objects} objects into {file_counter} files\")\n",
    "    \n",
    "    def split_multiple_paths(self, path_configs: List[Dict[str, Any]]) -> None:\n",
    "        \"\"\"\n",
    "        Split multiple nested paths into separate output directories.\n",
    "        \n",
    "        Args:\n",
    "            path_configs: List of dicts with 'path', 'output_subdir', 'chunk_size'\n",
    "        \"\"\"\n",
    "        file_opener = gzip.open if self.is_compressed else open\n",
    "        \n",
    "        # Initialize parsers and counters for each path\n",
    "        path_data = {}\n",
    "        for config in path_configs:\n",
    "            path_data[config['path']] = {\n",
    "                'config': config,\n",
    "                'chunk': [],\n",
    "                'file_counter': 1,\n",
    "                'total_objects': 0,\n",
    "                'output_dir': os.path.join(self.output_dir, config.get('output_subdir', config['path'].replace('.', '_')))\n",
    "            }\n",
    "            os.makedirs(path_data[config['path']]['output_dir'], exist_ok=True)\n",
    "        \n",
    "        with file_opener(self.input_file, 'rb') as file:\n",
    "            # Process each path separately\n",
    "            for path, data in path_data.items():\n",
    "                print(f\"Processing path: {path}\")\n",
    "                file.seek(0)  # Reset file position\n",
    "                \n",
    "                parser = ijson.items(file, path)\n",
    "                chunk_size = data['config'].get('chunk_size', 1000)\n",
    "                \n",
    "                for obj in parser:\n",
    "                    data['chunk'].append(obj)\n",
    "                    data['total_objects'] += 1\n",
    "                    \n",
    "                    if len(data['chunk']) >= chunk_size:\n",
    "                        self._write_chunk_to_dir(\n",
    "                            data['chunk'], \n",
    "                            data['output_dir'], \n",
    "                            data['file_counter']\n",
    "                        )\n",
    "                        data['chunk'] = []\n",
    "                        data['file_counter'] += 1\n",
    "                \n",
    "                # Write final chunk\n",
    "                if data['chunk']:\n",
    "                    self._write_chunk_to_dir(\n",
    "                        data['chunk'], \n",
    "                        data['output_dir'], \n",
    "                        data['file_counter']\n",
    "                    )\n",
    "                \n",
    "                print(f\"Path {path}: {data['total_objects']} objects in {data['file_counter']} files\")\n",
    "    \n",
    "    def split_by_nested_field(self, json_path: str, split_field: str, \n",
    "                             chunk_size: int = 1000) -> None:\n",
    "        \"\"\"\n",
    "        Split objects based on a nested field value.\n",
    "        \n",
    "        Args:\n",
    "            json_path: Path to the array items\n",
    "            split_field: Field to use for splitting (can be nested like 'user.type')\n",
    "            chunk_size: Objects per file per category\n",
    "        \"\"\"\n",
    "        file_opener = gzip.open if self.is_compressed else open\n",
    "        categories = {}\n",
    "        \n",
    "        with file_opener(self.input_file, 'rb') as file:\n",
    "            parser = ijson.items(file, json_path)\n",
    "            \n",
    "            for obj in parser:\n",
    "                # Extract split field value (supports nested fields)\n",
    "                field_value = self._get_nested_field(obj, split_field)\n",
    "                category = str(field_value) if field_value is not None else 'null'\n",
    "                \n",
    "                if category not in categories:\n",
    "                    categories[category] = {\n",
    "                        'objects': [],\n",
    "                        'file_counter': 1,\n",
    "                        'total_count': 0\n",
    "                    }\n",
    "                \n",
    "                categories[category]['objects'].append(obj)\n",
    "                categories[category]['total_count'] += 1\n",
    "                \n",
    "                # Write chunk if it's full\n",
    "                if len(categories[category]['objects']) >= chunk_size:\n",
    "                    self._write_category_chunk(\n",
    "                        categories[category]['objects'],\n",
    "                        category,\n",
    "                        categories[category]['file_counter']\n",
    "                    )\n",
    "                    categories[category]['objects'] = []\n",
    "                    categories[category]['file_counter'] += 1\n",
    "        \n",
    "        # Write remaining objects\n",
    "        for category, data in categories.items():\n",
    "            if data['objects']:\n",
    "                self._write_category_chunk(\n",
    "                    data['objects'],\n",
    "                    category,\n",
    "                    data['file_counter']\n",
    "                )\n",
    "            print(f\"Category '{category}': {data['total_count']} objects\")\n",
    "    \n",
    "    def _get_nested_field(self, obj: Dict, field_path: str) -> Any:\n",
    "        \"\"\"Get value from nested field path like 'user.profile.type'.\"\"\"\n",
    "        try:\n",
    "            value = obj\n",
    "            for field in field_path.split('.'):\n",
    "                value = value[field]\n",
    "            return value\n",
    "        except (KeyError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    def _write_flat_chunk(self, chunk: List, file_number: int) -> None:\n",
    "        \"\"\"Write chunk as flat array.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f'chunk_{file_number:04d}.json')\n",
    "        with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "            json.dump(chunk, out_file, cls=DecimalEncoder, separators=(',', ':'), ensure_ascii=False)\n",
    "        print(f\"Written flat chunk {file_number}: {len(chunk)} objects\")\n",
    "    \n",
    "    def _write_structured_chunk(self, chunk: List, json_path: str, file_number: int) -> None:\n",
    "        \"\"\"Write chunk preserving nested structure.\"\"\"\n",
    "        # Reconstruct nested structure\n",
    "        path_parts = json_path.replace('.item', '').split('.')\n",
    "        nested_data = chunk\n",
    "        \n",
    "        # Build nested structure from inside out\n",
    "        for part in reversed(path_parts):\n",
    "            nested_data = {part: nested_data}\n",
    "        \n",
    "        output_file = os.path.join(self.output_dir, f'structured_{file_number:04d}.json')\n",
    "        with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "            json.dump(nested_data, out_file, cls=DecimalEncoder, indent=2, ensure_ascii=False)\n",
    "        print(f\"Written structured chunk {file_number}: {len(chunk)} objects\")\n",
    "    \n",
    "    def _write_chunk_to_dir(self, chunk: List, output_dir: str, file_number: int) -> None:\n",
    "        \"\"\"Write chunk to specific directory.\"\"\"\n",
    "        output_file = os.path.join(output_dir, f'chunk_{file_number:04d}.json')\n",
    "        with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "            json.dump(chunk, out_file, cls=DecimalEncoder, separators=(',', ':'), ensure_ascii=False)\n",
    "    \n",
    "    def _write_category_chunk(self, chunk: List, category: str, file_number: int) -> None:\n",
    "        \"\"\"Write chunk for specific category.\"\"\"\n",
    "        safe_category = \"\".join(c for c in category if c.isalnum() or c in (' ', '-', '_')).strip()\n",
    "        category_dir = os.path.join(self.output_dir, f'category_{safe_category}')\n",
    "        os.makedirs(category_dir, exist_ok=True)\n",
    "        \n",
    "        output_file = os.path.join(category_dir, f'{safe_category}_{file_number:04d}.json')\n",
    "        with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "            json.dump(chunk, out_file, cls=DecimalEncoder ,separators=(',', ':'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0873205-8793-4931-ac1a-f511224a7ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Example usage functions\n",
    "# def example_nested_structures():\n",
    "#     \"\"\"Examples of handling different nested JSON structures.\"\"\"\n",
    "    \n",
    "#     # Example 1: Simple nested array\n",
    "#     # JSON structure: {\"data\": {\"items\": [...]}}\n",
    "#     splitter1 = NestedJSONGZSplitter('data_with_items.json.gz', 'output1')\n",
    "#     splitter1.split_nested_array('data.items.item', chunk_size=500)\n",
    "    \n",
    "#     # Example 2: Multiple nested arrays\n",
    "#     # JSON structure: {\"users\": [...], \"posts\": [...], \"comments\": [...]}\n",
    "#     splitter2 = NestedJSONGZSplitter('multi_data.json.gz', 'output2')\n",
    "#     path_configs = [\n",
    "#         {'path': 'users.item', 'output_subdir': 'users', 'chunk_size': 1000},\n",
    "#         {'path': 'posts.item', 'output_subdir': 'posts', 'chunk_size': 500},\n",
    "#         {'path': 'comments.item', 'output_subdir': 'comments', 'chunk_size': 2000}\n",
    "#     ]\n",
    "#     splitter2.split_multiple_paths(path_configs)\n",
    "    \n",
    "#     # Example 3: Deep nesting\n",
    "#     # JSON structure: {\"api\": {\"v1\": {\"responses\": {\"data\": [...]}}}}\n",
    "#     splitter3 = NestedJSONGZSplitter('deep_nested.json.gz', 'output3')\n",
    "#     splitter3.split_nested_array('api.v1.responses.data.item', chunk_size=100)\n",
    "    \n",
    "#     # Example 4: Split by nested field\n",
    "#     # Split based on user.account_type field\n",
    "#     splitter4 = NestedJSONGZSplitter('user_data.json.gz', 'output4')\n",
    "#     splitter4.split_by_nested_field('users.item', 'account.type', chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b7b67d2-92a3-476c-9e5e-b9b3a627e520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Analyze structure first\n",
    "#     splitter = NestedJSONGZSplitter('example.json.gz', 'output')\n",
    "#     structure_info = splitter.analyze_structure()\n",
    "#     print(\"Structure analysis:\", structure_info)\n",
    "    \n",
    "#     # Run examples\n",
    "#     example_nested_structures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e72575-7164-4bc7-bed2-28902e03eb9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # For nested JSON like: {\"data\": {\"records\": [...]}}\n",
    "#     split_nested_json('nested_data.json', 'output_nested', 'data.records.item', 100)\n",
    "    \n",
    "#     # Split by category field\n",
    "#     split_by_category('categorized_data.json', 'output_categories', 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87833efc-87c7-4c32-a6bb-7992b41614ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "splitter = NestedJSONGZSplitter(\n",
    "  input_file = nested_data\n",
    "  ,output_dir = output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aaae2a1-d840-467d-9320-f3d260d06da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "structure = splitter.analyze_structure()\n",
    "structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88671e80-ae06-4407-882e-f1d9054032b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "splitter.split_by_nested_field(\n",
    "  json_path = 'provider_references.item.provider_groups.item'\n",
    "  ,split_field = 'npi'\n",
    "  ,chunk_size = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ad8acd-bd82-45f9-b29e-5cea29a92dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Distributed JSON Splitter using Spark ---\n",
    "# This example demonstrates how to use Spark to process and split large nested JSON files in parallel.\n",
    "# It is designed for Databricks/Azure and can handle compressed files (e.g., .json.gz) natively.\n",
    "#\n",
    "# Adjust the paths and field names as needed for your data structure.\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Example input paths (update as needed)\n",
    "input_path = nested_data  # e.g., '/Volumes/.../in-network-rates_1_of_5.json.gz'\n",
    "output_base = output_dir # e.g., '/Volumes/.../split/'\n",
    "\n",
    "# 1. Read the JSON file (Spark can read .gz files directly)\n",
    "df = spark.read.option(\"multiline\", True).json(input_path)\n",
    "\n",
    "# 2. Inspect schema to find nested arrays\n",
    "# df.printSchema()\n",
    "\n",
    "# 3. Explode nested arrays (example: 'provider_references' is an array)\n",
    "# Adjust the explode path as needed for your JSON structure\n",
    "if 'provider_references' in df.columns:\n",
    "    exploded = df.select(F.explode('provider_references').alias('provider_reference'))\n",
    "else:\n",
    "    # If the array is nested deeper, adjust accordingly\n",
    "    # exploded = df.select(F.explode('data.items').alias('item'))\n",
    "    raise ValueError(\"Update the explode path for your JSON structure.\")\n",
    "\n",
    "# 4. Optionally, flatten nested fields for easier splitting\n",
    "flat = exploded.select(\n",
    "    'provider_reference.*'  # expand all fields of provider_reference\n",
    ")\n",
    "\n",
    "# 5. Split by a nested field (example: 'provider_groups')\n",
    "# If provider_groups is an array, explode it; otherwise, use as is\n",
    "if 'provider_groups' in flat.columns:\n",
    "    flat = flat.withColumn('provider_group', F.explode_outer('provider_groups'))\n",
    "else:\n",
    "    flat = flat.withColumn('provider_group', F.lit(None))\n",
    "\n",
    "# 6. Write out the results partitioned by provider_group (distributed write)\n",
    "# This will create one folder per provider_group value\n",
    "# flat.write \\\n",
    "#     .mode('overwrite') \\\n",
    "#     .partitionBy('provider_group') \\\n",
    "#     .json(f\"{output_base}/by_provider_group\")\n",
    "\n",
    "# 7. (Optional) If you want to split by chunk size instead, repartition and write\n",
    "chunk_size = 1000\n",
    "num_partitions = max(1, flat.count() // chunk_size)\n",
    "flat.repartition(num_partitions).write.mode('overwrite').json(f\"{output_base}/by_chunk\")\n",
    "\n",
    "# ---\n",
    "# Notes:\n",
    "# - Adjust the explode/select logic for your actual nested structure.\n",
    "# - For very large files, Spark will process and write in parallel across the cluster.\n",
    "# - You can use .parquet() instead of .json() for more efficient storage.\n",
    "# - For more complex splits (e.g., multiple fields), use .partitionBy with multiple columns.\n",
    "# - If you need to preserve more of the original structure, select/alias fields as needed before writing.\n",
    "\n",
    "# New code chunk demonstrating a Spark-native distributed splitter for nested JSON\n",
    "# This section provides an example of how to handle nested JSON data using Spark's DataFrame API.\n",
    "\n",
    "# 1. Read the nested JSON file\n",
    "nested_df = spark.read.json(input_path)\n",
    "\n",
    "# 2. Explode the nested field (e.g., 'items' within 'provider_references')\n",
    "if 'provider_references' in nested_df.columns:\n",
    "    exploded_nested = nested_df.select(F.explode('provider_references.items').alias('item'))\n",
    "else:\n",
    "    raise ValueError(\"The specified nested field does not exist.\")\n",
    "\n",
    "# 3. Flatten the exploded DataFrame to access nested fields\n",
    "flattened_df = exploded_nested.select(\n",
    "    'item.*'  # Select all fields from the exploded item\n",
    ")\n",
    "\n",
    "# 4. Write the flattened DataFrame to output, partitioned by a specific field if needed\n",
    "flattened_df.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy('some_field') \\  # Replace 'some_field' with the actual field to partition by\n",
    "    .json(f\"{output_base}/flattened_output\")\n",
    "\n",
    "# 5. (Optional) Repartition the DataFrame for better performance\n",
    "# flattened_df.repartition(100).write.mode('overwrite').json(f\"{output_base}/repartitioned_output\")\n",
    "\n",
    "# ---\n",
    "# This code chunk provides a basic framework for processing nested JSON data using Spark.\n",
    "# Adjust the field names and paths according to your specific data structure and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b428117-9626-48c4-9299-349d7d2c8efd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "ijson==3.4.*"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01 - Split Files",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
