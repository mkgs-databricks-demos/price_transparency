{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2779355-40cd-408a-9634-1f46d0e0103a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "from pyspark.sql.functions import pandas_udf, lit\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0acf0475-fd11-4b90-9e7b-406b18aa7bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_path = \"/Volumes/melissap/ardent_demo_bcbs_bronze/mrfs/in_network/2025-08_020_02E0_in-network-rates_2_of_2.json.gz?&Expires=1758376850&Signature=WnmraxwcwbZSbRQMY7aiZWbaAlCjKkF7R8tvTQHrASzbCCBFWcojQHcZpdGNaUY~dtTECuZYEJbNyBnItPnDhjR0w53R~aHIMa5MSkHNl66msesHo-ijchGHgdTEgkNxfoRPL4QTRw7iKlDE7rASmgFc9f8ZggxFNPc6-Mz1eWIrX2WoVaQI7tWWuV4FjJliGD9KxIitgiKHUZ09lATWjoKLGm25MdYiNEk01GVASymiMxG4DwWXgQvfi3wdqeSJCsk0QrO5wF4V-5hPrsIa3P1gHUg2NS1gaE4AzzH~WVMDnX-DwztvALLw4FE4I010RpSWyZ67lPX-pXEmuqLK9Q__&Key-Pair-Id=K27TQMT39R1C8A\"\n",
    "output_folder = \"/Volumes/mgiglia/dev_matthew_giglia_price_transparency/landing/in-network/uncompressed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "985d2079-7342-4e71-88b7-123af4dd2dfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def decompress_gzip_file(source_path, output_folder):\n",
    "    \"\"\"\n",
    "    Decompresses a gzipped file to a specified output folder, or copies the file if not compressed.\n",
    "    Cleans the filename by removing any characters to the right of .json.gz or .json.gzip.\n",
    "    Args:\n",
    "        source_path (str): Path to the gzipped file in Unity Catalog volume\n",
    "        output_folder (str): Destination folder for decompressed file\n",
    "    Returns:\n",
    "        tuple: (success_status, output_path_or_error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input parameters\n",
    "        if not source_path or not output_folder:\n",
    "            return False, \"Error: Source path or output folder is empty\"\n",
    "        # Check if source file exists\n",
    "        if not os.path.exists(source_path):\n",
    "            return False, f\"Error: Source file does not exist: {source_path}\"\n",
    "        # Clean the filename by removing query params and anything after .json.gz or .json.gzip\n",
    "        base_filename = os.path.basename(source_path)\n",
    "        output_filename = base_filename.lower().split(\"?\")[0]\n",
    "        if output_filename.endswith(('.gz', '.gzip')):\n",
    "            output_filename = output_filename.split(\".json.gz\")[0] + \".json\"\n",
    "        elif output_filename.endswith('.json'):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"Error: The file is not a JSON file\")\n",
    "            # # Remove query params if present\n",
    "            # output_filename = base_filename.split(\"?\")[0]\n",
    "        print(f\"\"\" \n",
    "              base_filename: {base_filename}\n",
    "              output_filename: {output_filename}\n",
    "        \"\"\")\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        # If file is compressed, decompress it\n",
    "        if \".gz\" in base_filename.lower() or \".gzip\" in base_filename.lower():\n",
    "        # if base_filename.lower().endswith(('.gz', '.gzip')):\n",
    "            try:\n",
    "                with gzip.open(source_path, 'rb') as gz_file:\n",
    "                    with open(output_path, 'wb') as output_file:\n",
    "                        shutil.copyfileobj(gz_file, output_file, length=1024*1024)  # 1MB chunks\n",
    "                # Verify the decompressed file was created and has content\n",
    "                if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "                    return True, output_path\n",
    "                else:\n",
    "                    return False, \"Error: Decompressed file is empty or was not created\"\n",
    "            except gzip.BadGzipFile:\n",
    "                return False, f\"Error: File is not a valid gzip file: {source_path}\"\n",
    "            except OSError as e:\n",
    "                return False, f\"Error: Failed to read gzip file - {str(e)}\"\n",
    "            except Exception as e:\n",
    "                return False, f\"Error during decompression: {str(e)}\"\n",
    "        else:\n",
    "            # If file is not compressed, just copy it to the output folder\n",
    "            try:\n",
    "                shutil.copy2(source_path, output_path)\n",
    "                if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "                    return True, output_path\n",
    "                else:\n",
    "                    return False, \"Error: Copied file is empty or was not created\"\n",
    "            except Exception as e:\n",
    "                return False, f\"Error during file copy: {str(e)}\"\n",
    "    except PermissionError:\n",
    "        return False, f\"Error: Permission denied accessing {source_path} or {output_folder}\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error: Unexpected error - {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2a336c-2dff-4368-af87-09a40cd15da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "decompress_gzip_file(source_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e363d6f-de09-4f46-be0d-23c9cf8748a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory to list files from\n",
    "directory = \"/Volumes/melissap/ardent_demo_bcbs_bronze/mrfs/in_network/\"\n",
    "\n",
    "# List all files (not directories) in the directory\n",
    "file_list = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "# Create Spark DataFrame\n",
    "df_files = spark.createDataFrame([(f,) for f in file_list], [\"file_path\"])\n",
    "\n",
    "display(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724e8f60-a361-4f9f-a6ff-f970795ac068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the return schema for the UDF\n",
    "decompress_result_schema = StructType([\n",
    "    StructField(\"success\", BooleanType(), True),\n",
    "    StructField(\"message\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f017d212-89c2-4d3a-b226-d32c7ab73477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=decompress_result_schema)\n",
    "def decompress_gzip_arrow_udf(source_paths: pd.Series, output_folders: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Arrow-optimized pandas UDF for batch processing of gzip decompression\n",
    "    Args:\n",
    "        source_paths: Pandas Series of source file paths\n",
    "        output_folders: Pandas Series of output folder paths\n",
    "    Returns:\n",
    "        DataFrame: Contains success and message columns\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for source_path, output_folder in zip(source_paths, output_folders):\n",
    "        success, message = decompress_gzip_file(source_path, output_folder)\n",
    "        results.append({'success': success, 'message': message})\n",
    "    df = pd.DataFrame(results)\n",
    "    # Ensure correct dtypes and non-nullable columns\n",
    "    df['success'] = df['success'].astype(bool).fillna(False)\n",
    "    df['message'] = df['message'].astype(str).fillna('')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72edffcc-16ff-4389-b4a3-1fe49caf135d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def apply_gzip_decompression_arrow(df, source_column, output_folder_path):\n",
    "    \"\"\"\n",
    "    Apply the Arrow-optimized decompression UDF to a DataFrame\n",
    "    \"\"\"\n",
    "    result_df = df.withColumn(\n",
    "        \"decompress_result\",\n",
    "        decompress_gzip_arrow_udf(\n",
    "            col(source_column),\n",
    "            lit(output_folder_path)\n",
    "        )\n",
    "    )\n",
    "    # # Extract success and message from struct\n",
    "    # final_df = result_df.select(\n",
    "    #     \"*\",\n",
    "    #     col(\"decompress_result.success\").alias(\"decompress_success\"),\n",
    "    #     col(\"decompress_result.message\").alias(\"decompress_message\")\n",
    "    # ).drop(\"decompress_result\")\n",
    "    # return final_df\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "299e1955-c76b-4a60-9d5f-ab15dbb4c821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = apply_gzip_decompression_arrow(\n",
    "  df = df_files\n",
    "  ,source_column = \"file_path\"\n",
    "  ,output_folder_path=output_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b97fcdb5-22d9-4886-a371-2bce87e4768a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"file_path\":907},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757263292612}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c94b846c-d7f0-4e12-968a-5d4584e71b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Define the return schema for the UDF\n",
    "# decompress_result_schema = StructType([\n",
    "#     StructField(\"success\", BooleanType(), False),\n",
    "#     StructField(\"message\", StringType(), False)\n",
    "# ])\n",
    "# # Create the UDF with proper Arrow optimization\n",
    "# @udf(returnType=decompress_result_schema)\n",
    "# def decompress_gzip_udf(source_path, output_folder):\n",
    "#     \"\"\"\n",
    "#     Spark UDF wrapper for the decompress function\n",
    "#     Args:\n",
    "#         source_path (str): Path to the gzipped file\n",
    "#         output_folder (str): Destination folder for decompressed file\n",
    "#     Returns:\n",
    "#         struct: Contains success boolean and message (output path or error)\n",
    "#     \"\"\"\n",
    "#     success, message = decompress_gzip_file(source_path, output_folder)\n",
    "#     return (success, message)\n",
    "\n",
    "# # Alternative version using Arrow-optimized pandas UDF for better performance\n",
    "# @pandas_udf(returnType=decompress_result_schema)\n",
    "# def decompress_gzip_arrow_udf(source_paths: pd.Series, output_folders: pd.Series) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Arrow-optimized pandas UDF for batch processing of gzip decompression\n",
    "#     Args:\n",
    "#         source_paths: Pandas Series of source file paths\n",
    "#         output_folders: Pandas Series of output folder paths\n",
    "#     Returns:\n",
    "#         DataFrame: Contains success and message columns\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "#     for source_path, output_folder in zip(source_paths, output_folders):\n",
    "#         success, message = decompress_gzip_file(source_path, output_folder)\n",
    "#         results.append({'success': success, 'message': message})\n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# # Example usage with your DataFrame\n",
    "# def apply_gzip_decompression(df, source_column, output_folder_path):\n",
    "#     \"\"\"\n",
    "#     Apply the decompression UDF to a DataFrame\n",
    "#     Args:\n",
    "#         df: Spark DataFrame with file paths\n",
    "#         source_column: Name of column containing gzip file paths\n",
    "#         output_folder_path: Path where decompressed files should be stored\n",
    "#     Returns:\n",
    "#         DataFrame: Original DataFrame with added result struct column\n",
    "#     \"\"\"\n",
    "#     # Using regular UDF\n",
    "#     result_df = df.withColumn(\n",
    "#         \"decompress_result\",\n",
    "#         decompress_gzip_udf(\n",
    "#             col(source_column),\n",
    "#             lit(output_folder_path)\n",
    "#         )\n",
    "#     )\n",
    "#     # Extract success and message from struct for easier access\n",
    "#     final_df = result_df.select(\n",
    "#         \"*\",\n",
    "#         col(\"decompress_result.success\").alias(\"decompress_success\"),\n",
    "#         col(\"decompress_result.message\").alias(\"decompress_message\")\n",
    "#     ).drop(\"decompress_result\")\n",
    "#     return final_df\n",
    "\n",
    "# # Alternative usage with Arrow-optimized UDF\n",
    "# def apply_gzip_decompression_arrow(df, source_column, output_folder_path):\n",
    "#     \"\"\"\n",
    "#     Apply the Arrow-optimized decompression UDF to a DataFrame\n",
    "#     \"\"\"\n",
    "#     result_df = df.withColumn(\n",
    "#         \"decompress_result\",\n",
    "#         decompress_gzip_arrow_udf(\n",
    "#             col(source_column),\n",
    "#             lit(output_folder_path)\n",
    "#         )\n",
    "#     )\n",
    "#     # Extract success and message from struct\n",
    "#     final_df = result_df.select(\n",
    "#         \"*\",\n",
    "#         col(\"decompress_result.success\").alias(\"decompress_success\"),\n",
    "#         col(\"decompress_result.message\").alias(\"decompress_message\")\n",
    "#     ).drop(\"decompress_result\")\n",
    "#     return final_df\n",
    "# # Example usage:\n",
    "# \"\"\"\n",
    "# # Assuming you have a DataFrame with a column 'file_paths' containing gzipped file locations\n",
    "# df = spark.createDataFrame([\n",
    "#     (\"/Volumes/catalog/schema/volume/data1.gz\",),\n",
    "#     (\"/Volumes/catalog/schema/volume/data2.gz\",),\n",
    "#     (\"/Volumes/catalog/schema/volume/invalid.txt\",),\n",
    "# ], [\"file_paths\"])\n",
    "# # Apply decompression\n",
    "# output_folder = \"/Volumes/catalog/schema/volume/decompressed/\"\n",
    "# result_df = apply_gzip_decompression_arrow(df, \"file_paths\", output_folder)\n",
    "# # Show results\n",
    "# result_df.select(\"file_paths\", \"decompress_success\", \"decompress_message\").show(truncate=False)\n",
    "# \"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "/Workspace/Users/matthew.giglia@databricks.com/price_transparency/price_transparency/resources/index_pipeline/environment_v4.yaml",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "decompress files",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
