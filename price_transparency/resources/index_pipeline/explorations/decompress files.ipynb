{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c94b846c-d7f0-4e12-968a-5d4584e71b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "def decompress_gzip_file(source_path, output_folder):\n",
    "    \"\"\"\n",
    "    Decompresses a gzipped file to a specified output folder\n",
    "    Args:\n",
    "        source_path (str): Path to the gzipped file in Unity Catalog volume\n",
    "        output_folder (str): Destination folder for decompressed file\n",
    "    Returns:\n",
    "        tuple: (success_status, output_path_or_error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input parameters\n",
    "        if not source_path or not output_folder:\n",
    "            return False, \"Error: Source path or output folder is empty\"\n",
    "        # Check if source file exists\n",
    "        if not os.path.exists(source_path):\n",
    "            return False, f\"Error: Source file does not exist: {source_path}\"\n",
    "        # Check if file has .gz extension\n",
    "        if not source_path.lower().endswith(('.gz', '.gzip')):\n",
    "            return False, f\"Error: File is not a gzip file (missing .gz/.gzip extension): {source_path}\"\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        # Generate output filename (remove .gz extension)\n",
    "        base_filename = os.path.basename(source_path)\n",
    "        if base_filename.endswith('.gz'):\n",
    "            output_filename = base_filename[:-3]  # Remove .gz\n",
    "        elif base_filename.endswith('.gzip'):\n",
    "            output_filename = base_filename[:-5]  # Remove .gzip\n",
    "        else:\n",
    "            output_filename = base_filename + \"_decompressed\"\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        # Decompress the file using chunked reading for large files\n",
    "        try:\n",
    "            with gzip.open(source_path, 'rb') as gz_file:\n",
    "                with open(output_path, 'wb') as output_file:\n",
    "                    # Use shutil.copyfileobj for efficient chunked copying\n",
    "                    shutil.copyfileobj(gz_file, output_file, length=1024*1024)  # 1MB chunks\n",
    "            # Verify the decompressed file was created and has content\n",
    "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "                return True, output_path\n",
    "            else:\n",
    "                return False, \"Error: Decompressed file is empty or was not created\"\n",
    "        except gzip.BadGzipFile:\n",
    "            return False, f\"Error: File is not a valid gzip file: {source_path}\"\n",
    "        except OSError as e:\n",
    "            return False, f\"Error: Failed to read gzip file - {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Error during decompression: {str(e)}\"\n",
    "    except PermissionError:\n",
    "        return False, f\"Error: Permission denied accessing {source_path} or {output_folder}\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error: Unexpected error - {str(e)}\"\n",
    "# Define the return schema for the UDF\n",
    "decompress_result_schema = StructType([\n",
    "    StructField(\"success\", BooleanType(), False),\n",
    "    StructField(\"message\", StringType(), False)\n",
    "])\n",
    "# Create the UDF with proper Arrow optimization\n",
    "@udf(returnType=decompress_result_schema)\n",
    "def decompress_gzip_udf(source_path, output_folder):\n",
    "    \"\"\"\n",
    "    Spark UDF wrapper for the decompress function\n",
    "    Args:\n",
    "        source_path (str): Path to the gzipped file\n",
    "        output_folder (str): Destination folder for decompressed file\n",
    "    Returns:\n",
    "        struct: Contains success boolean and message (output path or error)\n",
    "    \"\"\"\n",
    "    success, message = decompress_gzip_file(source_path, output_folder)\n",
    "    return (success, message)\n",
    "# Alternative version using Arrow-optimized pandas UDF for better performance\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "@pandas_udf(returnType=decompress_result_schema)\n",
    "def decompress_gzip_arrow_udf(source_paths: pd.Series, output_folders: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Arrow-optimized pandas UDF for batch processing of gzip decompression\n",
    "    Args:\n",
    "        source_paths: Pandas Series of source file paths\n",
    "        output_folders: Pandas Series of output folder paths\n",
    "    Returns:\n",
    "        DataFrame: Contains success and message columns\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for source_path, output_folder in zip(source_paths, output_folders):\n",
    "        success, message = decompress_gzip_file(source_path, output_folder)\n",
    "        results.append({'success': success, 'message': message})\n",
    "    return pd.DataFrame(results)\n",
    "# Example usage with your DataFrame\n",
    "def apply_gzip_decompression(df, source_column, output_folder_path):\n",
    "    \"\"\"\n",
    "    Apply the decompression UDF to a DataFrame\n",
    "    Args:\n",
    "        df: Spark DataFrame with file paths\n",
    "        source_column: Name of column containing gzip file paths\n",
    "        output_folder_path: Path where decompressed files should be stored\n",
    "    Returns:\n",
    "        DataFrame: Original DataFrame with added result struct column\n",
    "    \"\"\"\n",
    "    # Using regular UDF\n",
    "    result_df = df.withColumn(\n",
    "        \"decompress_result\",\n",
    "        decompress_gzip_udf(\n",
    "            col(source_column),\n",
    "            lit(output_folder_path)\n",
    "        )\n",
    "    )\n",
    "    # Extract success and message from struct for easier access\n",
    "    final_df = result_df.select(\n",
    "        \"*\",\n",
    "        col(\"decompress_result.success\").alias(\"decompress_success\"),\n",
    "        col(\"decompress_result.message\").alias(\"decompress_message\")\n",
    "    ).drop(\"decompress_result\")\n",
    "    return final_df\n",
    "# Alternative usage with Arrow-optimized UDF\n",
    "def apply_gzip_decompression_arrow(df, source_column, output_folder_path):\n",
    "    \"\"\"\n",
    "    Apply the Arrow-optimized decompression UDF to a DataFrame\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import lit\n",
    "    result_df = df.withColumn(\n",
    "        \"decompress_result\",\n",
    "        decompress_gzip_arrow_udf(\n",
    "            col(source_column),\n",
    "            lit(output_folder_path)\n",
    "        )\n",
    "    )\n",
    "    # Extract success and message from struct\n",
    "    final_df = result_df.select(\n",
    "        \"*\",\n",
    "        col(\"decompress_result.success\").alias(\"decompress_success\"),\n",
    "        col(\"decompress_result.message\").alias(\"decompress_message\")\n",
    "    ).drop(\"decompress_result\")\n",
    "    return final_df\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Assuming you have a DataFrame with a column 'file_paths' containing gzipped file locations\n",
    "df = spark.createDataFrame([\n",
    "    (\"/Volumes/catalog/schema/volume/data1.gz\",),\n",
    "    (\"/Volumes/catalog/schema/volume/data2.gz\",),\n",
    "    (\"/Volumes/catalog/schema/volume/invalid.txt\",),\n",
    "], [\"file_paths\"])\n",
    "# Apply decompression\n",
    "output_folder = \"/Volumes/catalog/schema/volume/decompressed/\"\n",
    "result_df = apply_gzip_decompression_arrow(df, \"file_paths\", output_folder)\n",
    "# Show results\n",
    "result_df.select(\"file_paths\", \"decompress_success\", \"decompress_message\").show(truncate=False)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "/Workspace/Users/matthew.giglia@databricks.com/price_transparency/price_transparency/resources/index_pipeline/environment_v4.yaml",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "decompress files",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
